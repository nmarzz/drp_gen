{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-israeli",
   "metadata": {},
   "source": [
    "# Thermodynamics Diffusion Model\n",
    "\n",
    "This code is a heavy revamp of the code provided [by JM Tomczak](https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html)\n",
    "\n",
    "In particular\n",
    "- It appears that their is a bug in the loss of the provided code. Namely incorrectly calculating KL-div\n",
    "- I'm not sure their code actually trains the backward diffusion models\n",
    "- Heavy reformatting and additions\n",
    "\n",
    "\n",
    "\n",
    "There are a number of things to do\n",
    "- I'm sure we can get a better model architecture \n",
    "- Implement a non-constant beta\n",
    "- Attempt the only learn the mean function (ie, don't learn backward variance and set it equal to beta)\n",
    "- implement the swiss roll example\n",
    "    - This will provide nice plots and proof of concept\n",
    "- Show that the loss shown in the above page is equivalent to the one in \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\"\n",
    "    - If it is not, how is it different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lasting-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "PI = torch.from_numpy(np.asarray(np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latest-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, beta : int, data_dim:int, T : int, model_dim : int):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        # This model reconstructs our input x from the last value of our latent variable\n",
    "        self.decoder_net = nn.Sequential(nn.Linear(data_dim, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, data_dim))\n",
    "        \n",
    "        # These models predict the mean and standard deviation of the backward process\n",
    "        self.backward_models = nn.ModuleList([nn.Sequential(nn.Linear(data_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, 2 * data_dim)) for _ in range(T-1)])\n",
    "\n",
    "\n",
    "        # Data dimension\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "        # How many time steps are we taking\n",
    "        self.T = T\n",
    "        \n",
    "        self.register_buffer('beta', torch.tensor([beta], dtype = torch.float32))\n",
    "\n",
    "    def backward_diffusion_step(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward_diffusion_step(self, x, i):\n",
    "        return torch.sqrt(1. - self.beta) * x + torch.sqrt(self.beta) * torch.randn_like(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward difussion\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        # backward diffusion\n",
    "        mus = []\n",
    "        log_vars = []\n",
    "\n",
    "        for i in range(len(self.backward_models) - 1, -1, -1):\n",
    "            h = self.backward_models[i](zs[i+1])\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            mus.append(mu_i)\n",
    "            log_vars.append(log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(zs[0])\n",
    "                \n",
    "        return mu_x, mus, log_vars, zs        \n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        z = torch.randn([batch_size, self.data_dim]).to(device) * self.beta\n",
    "        for i in range(len(self.backward_models) - 1, -1, -1):\n",
    "            h = self.backward_models[i](z)\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            z = self.backward_diffusion_step(mu_i, log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(z)\n",
    "\n",
    "        return mu_x\n",
    "\n",
    "    def sample_diffusion(self, x):\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        return zs[-1]\n",
    "    \n",
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "            \n",
    "def mnist_loader(batch_size: int):\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))                                   \n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "                             ]), download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, pin_memory=True, shuffle = True)\n",
    "\n",
    "    test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory= True)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hungarian-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions\n",
    "def log_normal_diag(x, mu, log_var):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    return log_p\n",
    "\n",
    "def log_standard_normal(x):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.    \n",
    "    return log_p\n",
    "\n",
    "#### Loss function\n",
    "def calculate_ELBO(x, mu_x, mus, log_vars, zs, reduction = 'avg'):\n",
    "    reconstruction = F.mse_loss(x,mu_x)\n",
    "\n",
    "    # KL\n",
    "    log_q = log_normal_diag(zs[-1], torch.sqrt(1. - model.beta) * zs[-1], torch.log(model.beta))\n",
    "    q = torch.exp(log_q)\n",
    "    loq_std_normal =  log_standard_normal(zs[-1])\n",
    "    KL = (q * (log_q - loq_std_normal)).sum(-1)\n",
    "\n",
    "    for i in range(len(mus)):\n",
    "        log_q = log_normal_diag(zs[i], torch.sqrt(1. - model.beta) * zs[i], torch.log(model.beta))\n",
    "        q = torch.exp(log_q)\n",
    "        log_p = log_normal_diag(zs[i], mus[i], log_vars[i])\n",
    "        KL_i = (q * (log_q - log_p)).sum(-1)\n",
    "\n",
    "        KL += KL_i\n",
    "\n",
    "    # Final ELBO\n",
    "    if reduction == 'sum':\n",
    "        loss = (reconstruction + KL).sum()\n",
    "    else:\n",
    "        loss = (reconstruction + KL).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "auburn-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()\n",
    "    for data, _ in train_loader:    \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss = calculate_ELBO(data, mu_x,mus,log_vars,latents)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    N = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss_t = calculate_ELBO(data, mu_x,mus,log_vars,latents, reduction='sum')\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, NLL = {loss}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fitting-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, NLL = 76.31643259390684\n",
      "Epoch: 10, NLL = 39.84781400711653\n",
      "Epoch: 20, NLL = 29.885839215784777\n",
      "Epoch: 30, NLL = 27.77300328502392\n",
      "Epoch: 40, NLL = 22.018288797835634\n",
      "Epoch: 50, NLL = 24.89288467924149\n",
      "Epoch: 60, NLL = 19.489972697297482\n",
      "Epoch: 70, NLL = 14.74119826332389\n"
     ]
    }
   ],
   "source": [
    "### Define datasets\n",
    "\n",
    "dataset = 'digits'\n",
    "# dataset = 'mnist'\n",
    "batch_size = 64\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 10\n",
    "elif dataset == 'mnist':     \n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 1\n",
    "            \n",
    "\n",
    "## Define hyperparameters\n",
    "hidden_size = 256 # size of layers in model\n",
    "lr = 1e-3 \n",
    "T = 5 # Number diffusion steps\n",
    "beta = 0.6 # Diffusion coefficient \n",
    "early_stop = 40\n",
    "num_epochs = 80\n",
    "\n",
    "### Define model and optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DiffusionModel(beta = beta , data_dim = data_dim, T = T, model_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        epochs_since_improvement = 0\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement > early_stop:\n",
    "            break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "according-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "num_x = 4\n",
    "num_y = 4\n",
    "x = model.sample(batch_size=num_x * num_y, device = device)\n",
    "x = x.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(num_x, num_y)\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    plottable_image = np.reshape(x[i], (data_width, data_width))\n",
    "    ax.imshow(plottable_image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
