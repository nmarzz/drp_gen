{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-israeli",
   "metadata": {},
   "source": [
    "# Thermodynamics Diffusion Model\n",
    "\n",
    "This code is a heavy revamp of the code provided [by JM Tomczak](https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html)\n",
    "\n",
    "In particular\n",
    "- It appears that their is a bug in the loss of the provided code. Namely incorrectly calculating KL-div\n",
    "- I'm not sure their code actually trains the backward diffusion models\n",
    "- Heavy reformatting and additions\n",
    "\n",
    "\n",
    "\n",
    "There are a number of things to do\n",
    "- I'm sure we can get a better model architecture \n",
    "- Implement a non-constant beta\n",
    "- Attempt the only learn the mean function (ie, don't learn backward variance and set it equal to beta)\n",
    "- implement the swiss roll example\n",
    "    - This will provide nice plots and proof of concept\n",
    "- Show that the loss shown in the above page is equivalent to the one in \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\"\n",
    "    - If it is not, how is it different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lasting-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "PI = torch.from_numpy(np.asarray(np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "latest-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, beta : int, data_dim:int, T : int, model_dim : int):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        # This model reconstructs our input x from the last value of our latent variable\n",
    "        self.decoder_net = nn.Sequential(nn.Linear(data_dim, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, data_dim))\n",
    "        \n",
    "        # These models predict the mean and standard deviation of the backward process\n",
    "        self.backward_models = nn.ModuleList([nn.Sequential(nn.Linear(data_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, 2 * data_dim)) for _ in range(T-1)])\n",
    "\n",
    "\n",
    "        # Data dimension\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "        # How many time steps are we taking\n",
    "        self.T = T\n",
    "        \n",
    "        # self.register_buffer('beta', torch.tensor(beta, dtype = torch.float32))\n",
    "        self.beta = torch.tensor(np.linspace(beta, 0.3, T)) # add a beta list\n",
    "        # self.beta = torch.tensor([beta]*T)\n",
    "\n",
    "    def backward_diffusion_step(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward_diffusion_step(self, x, i):\n",
    "        return torch.sqrt(1. - self.beta[i]) * x + torch.sqrt(self.beta[i]) * torch.randn_like(x) # use beta[i]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward difussion\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        # backward diffusion\n",
    "        mus = []\n",
    "        log_vars = []\n",
    "\n",
    "        for i in range(len(self.backward_models)-1, -1, -1): \n",
    "            h = self.backward_models[i](zs[i+1])\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            mus.append(mu_i)\n",
    "            log_vars.append(log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(zs[0])\n",
    "                \n",
    "        return mu_x, mus, log_vars, zs        \n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        z = torch.randn([batch_size, self.data_dim]).to(device) * self.beta[self.T-1]\n",
    "        for i in range(len(self.backward_models) - 1, -1, -1):\n",
    "            h = self.backward_models[i](z)\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            z = self.backward_diffusion_step(mu_i, log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(z)\n",
    "\n",
    "        return mu_x\n",
    "\n",
    "    def sample_diffusion(self, x):\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        return zs[-1]\n",
    "    \n",
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "            \n",
    "def mnist_loader(batch_size: int):\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))                                   \n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "                             ]), download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, pin_memory=True, shuffle = True)\n",
    "\n",
    "    test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory= True)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hungarian-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions\n",
    "def log_normal_diag(x, mu, log_var):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    return log_p\n",
    "\n",
    "def log_standard_normal(x):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.    \n",
    "    return log_p\n",
    "\n",
    "#### Loss function\n",
    "def calculate_ELBO(x, mu_x, mus, log_vars, zs, reduction = 'avg'):\n",
    "    reconstruction = F.mse_loss(x,mu_x)\n",
    "\n",
    "    # KL\n",
    "    log_q = log_normal_diag(zs[-1], torch.sqrt(1. - model.beta[-1]) * zs[-1], torch.log(model.beta[-1]))\n",
    "    q = torch.exp(log_q)\n",
    "    loq_std_normal =  log_standard_normal(zs[-1])\n",
    "    KL = (q * (log_q - loq_std_normal)).sum(-1)\n",
    "\n",
    "    for i in range(len(mus)): # length: T-1\n",
    "        log_q = log_normal_diag(zs[i], torch.sqrt(1. - model.beta[i]) * zs[i], torch.log(model.beta[i]))\n",
    "        q = torch.exp(log_q)\n",
    "        log_p = log_normal_diag(zs[i], mus[i], log_vars[i])\n",
    "        KL_i = (q * (log_q - log_p)).sum(-1)\n",
    "\n",
    "        KL += KL_i\n",
    "\n",
    "    # Final ELBO\n",
    "    if reduction == 'sum':\n",
    "        loss = (reconstruction + KL).sum()\n",
    "    else:\n",
    "        loss = (reconstruction + KL).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "auburn-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()\n",
    "    for data, _ in train_loader:    \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss = calculate_ELBO(data, mu_x,mus,log_vars,latents)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    N = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss_t = calculate_ELBO(data, mu_x,mus,log_vars,latents, reduction='sum')\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, NLL = {loss}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fitting-skating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, NLL = 150.17025331810697\n",
      "Epoch: 10, NLL = 136.6785798306148\n",
      "Epoch: 20, NLL = 131.56822765546582\n",
      "Epoch: 30, NLL = 128.93520808010507\n",
      "Epoch: 40, NLL = 127.87280334702399\n",
      "Epoch: 50, NLL = 126.27358208516311\n",
      "Epoch: 60, NLL = 126.83747494265606\n",
      "Epoch: 70, NLL = 126.46256040719103\n"
     ]
    }
   ],
   "source": [
    "### Define datasets\n",
    "\n",
    "dataset = 'digits'\n",
    "# dataset = 'mnist'\n",
    "batch_size = 64\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 10\n",
    "elif dataset == 'mnist':     \n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 1\n",
    "            \n",
    "\n",
    "## Define hyperparameters\n",
    "hidden_size = 128 # size of layers in model\n",
    "lr = 1e-3\n",
    "T = 6 # Number diffusion steps\n",
    "beta = 0.7 # Diffusion coefficient \n",
    "early_stop = 40\n",
    "num_epochs = 80\n",
    "\n",
    "### Define model and optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DiffusionModel(beta = beta , data_dim = data_dim, T = T, model_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        epochs_since_improvement = 0\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement > early_stop:\n",
    "            break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "according-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "num_x = 4\n",
    "num_y = 4\n",
    "x = model.sample(batch_size=num_x * num_y, device = device)\n",
    "x = x.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(num_x, num_y)\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    plottable_image = np.reshape(x[i], (data_width, data_width))\n",
    "    ax.imshow(plottable_image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
