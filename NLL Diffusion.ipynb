{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-dialogue",
   "metadata": {},
   "source": [
    "# Thermodynamics Diffusion Model\n",
    "\n",
    "This code is a heavy revamp of the code provided [by JM Tomczak](https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html)\n",
    "\n",
    "In particular\n",
    "- It appears that their is a bug in the loss of the provided code. Namely incorrectly calculating KL-div\n",
    "- I'm not sure their code actually trains the backward diffusion models\n",
    "- Heavy reformatting and additions\n",
    "\n",
    "\n",
    "\n",
    "There are a number of things to do\n",
    "- Implement a non-constant beta\n",
    "- Attempt the only learn the mean function (ie, don't learn backward variance and set it equal to beta)\n",
    "- implement the swiss roll example\n",
    "    - This will provide nice plots and proof of concept\n",
    "- Show that the loss shown in the above page is equivalent to the one in \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\"\n",
    "    - If it is not, how is it different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caroline-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "PI = torch.from_numpy(np.asarray(np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, beta : int, data_dim:int, T : int, model_dim : int):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        # This model reconstructs our input x from the last value of our latent variable\n",
    "        self.decoder_net = nn.Sequential(nn.Linear(data_dim, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, model_dim*2), nn.SiLU(),\n",
    "                                    nn.Linear(model_dim*2, data_dim))\n",
    "        \n",
    "        # These models predict the mean and standard deviation of the backward process\n",
    "        self.backward_models = nn.ModuleList([nn.Sequential(nn.Linear(data_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, model_dim), nn.SiLU(),\n",
    "                        nn.Linear(model_dim, 2 * data_dim)) for _ in range(T-1)])\n",
    "\n",
    "\n",
    "        # Data dimension\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "        # How many time steps are we taking\n",
    "        self.T = T\n",
    "        \n",
    "        self.register_buffer('beta', torch.tensor([beta], dtype = torch.float32))\n",
    "\n",
    "    def backward_diffusion_step(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward_diffusion_step(self, x, i):\n",
    "        return torch.sqrt(1. - self.beta) * x + torch.sqrt(self.beta) * torch.randn_like(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward difussion\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        # backward diffusion\n",
    "        mus = []\n",
    "        log_vars = []\n",
    "\n",
    "        for i in range(len(self.backward_models) - 1, -1, -1):\n",
    "            h = self.backward_models[i](zs[i+1])\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            mus.append(mu_i)\n",
    "            log_vars.append(log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(zs[0])\n",
    "                \n",
    "        return mu_x, mus, log_vars, zs        \n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        z = torch.randn([batch_size, self.data_dim]).to(device) * self.beta\n",
    "        for i in range(len(self.backward_models) - 1, -1, -1):\n",
    "            h = self.backward_models[i](z)\n",
    "            mu_i, log_var_i = torch.chunk(h, 2, dim=1)\n",
    "            z = self.backward_diffusion_step(mu_i, log_var_i)\n",
    "\n",
    "        mu_x = self.decoder_net(z)\n",
    "\n",
    "        return mu_x\n",
    "\n",
    "    def sample_diffusion(self, x):\n",
    "        zs = [self.forward_diffusion_step(x, 0)]\n",
    "\n",
    "        for i in range(1, self.T):\n",
    "            zs.append(self.forward_diffusion_step(zs[-1], i))\n",
    "\n",
    "        return zs[-1]\n",
    "    \n",
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1000:].astype(np.float32)\n",
    "\n",
    "        self.targets = digits.target\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample, target    \n",
    "\n",
    "def digits_loader(batch_size:int):\n",
    "    transform = transforms.Lambda(lambda x: 2. * (x / 17.) - 1.)\n",
    "    train_data = Digits(mode='train', transforms=transform)\n",
    "    test_data = Digits(mode='test', transforms=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory= True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "            \n",
    "def mnist_loader(batch_size: int):\n",
    "    train_set = datasets.MNIST('data', train=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))                                   \n",
    "                               ]), download=True)\n",
    "    val_set = datasets.MNIST('data', train=False,\n",
    "                             transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "                             ]), download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "    test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory= True)\n",
    "\n",
    "    return train_loader, test_loader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "warming-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions\n",
    "def log_normal_diag(x, mu, log_var):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    return log_p\n",
    "\n",
    "def log_standard_normal(x):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.    \n",
    "    return log_p\n",
    "\n",
    "#### Loss function\n",
    "def calculate_ELBO(x, mu_x, mus, log_vars, zs, reduction = 'avg'):\n",
    "    reconstruction = F.mse_loss(x,mu_x)\n",
    "\n",
    "    # KL\n",
    "    log_q = log_normal_diag(zs[-1], torch.sqrt(1. - model.beta) * zs[-1], torch.log(model.beta))\n",
    "    q = torch.exp(log_q)\n",
    "    loq_std_normal =  log_standard_normal(zs[-1])\n",
    "    KL = (q * (log_q - loq_std_normal)).sum(-1)\n",
    "\n",
    "    for i in range(len(mus)):\n",
    "        log_q = log_normal_diag(zs[i], torch.sqrt(1. - model.beta) * zs[i], torch.log(model.beta))\n",
    "        q = torch.exp(log_q)\n",
    "        log_p = log_normal_diag(zs[i], mus[i], log_vars[i])\n",
    "        KL_i = (q * (log_q - log_p)).sum(-1)\n",
    "\n",
    "        KL += KL_i\n",
    "\n",
    "    # Final ELBO\n",
    "    if reduction == 'sum':\n",
    "        loss = (reconstruction + KL).sum()\n",
    "    else:\n",
    "        loss = (reconstruction + KL).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, train_loader):    \n",
    "    model.train()\n",
    "    for data, _ in train_loader:    \n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss = calculate_ELBO(data, mu_x,mus,log_vars,latents)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "def test_epoch(epoch, test_loader, output_epochs = 10):        \n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    N = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        mu_x, mus, log_vars, latents = model(data)\n",
    "        \n",
    "        loss_t = calculate_ELBO(data, mu_x,mus,log_vars,latents, reduction='sum')\n",
    "\n",
    "        loss += loss_t.item()\n",
    "        N += data.shape[0]\n",
    "\n",
    "    loss = loss / N\n",
    "    \n",
    "    if epoch % output_epochs == 0:\n",
    "        print(f'Epoch: {epoch}, NLL = {loss}')\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, NLL = 76.31643259390684\n",
      "Epoch: 10, NLL = 39.84781400711653\n",
      "Epoch: 20, NLL = 29.885839215784777\n"
     ]
    }
   ],
   "source": [
    "### Define datasets\n",
    "\n",
    "dataset = 'digits'\n",
    "# dataset = 'mnist'\n",
    "batch_size = 64\n",
    "\n",
    "if dataset == 'digits':\n",
    "    train_loader, test_loader = digits_loader(batch_size = batch_size)\n",
    "    data_width = 8\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 10\n",
    "elif dataset == 'mnist':     \n",
    "    train_loader, test_loader = mnist_loader(batch_size = batch_size)\n",
    "    data_width = 28\n",
    "    data_dim = data_width**2\n",
    "    output_epochs = 1\n",
    "            \n",
    "\n",
    "## Define hyperparameters\n",
    "hidden_size = 256 # size of layers in model\n",
    "lr = 1e-3 \n",
    "T = 5 # Number diffusion steps\n",
    "beta = 0.6 # Diffusion coefficient \n",
    "early_stop = 40\n",
    "num_epochs = 80\n",
    "\n",
    "### Define model and optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DiffusionModel(beta = beta , data_dim = data_dim, T = T, model_dim = hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = 1000\n",
    "epochs_since_improvement = 0\n",
    "for epoch in range(num_epochs):    \n",
    "    train_epoch(epoch, train_loader)\n",
    "    test_loss = test_epoch(epoch, test_loader, output_epochs = output_epochs)\n",
    "    if test_loss < best_loss:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        epochs_since_improvement = 0\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement > early_stop:\n",
    "            break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "num_x = 4\n",
    "num_y = 4\n",
    "x = model.sample(batch_size=num_x * num_y, device = device)\n",
    "x = x.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(num_x, num_y)\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    plottable_image = np.reshape(x[i], (data_width, data_width))\n",
    "    ax.imshow(plottable_image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
